{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis via Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/databeast03/anaconda3/lib/python3.6/site-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/databeast03/anaconda3/lib/python3.6/site-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.cross_validation import train_test_split, ShuffleSplit\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk import RegexpTokenizer\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import os\n",
    "from pprint import pprint\n",
    "from sklearn.grid_search import ParameterGrid\n",
    "from ipyparallel import Client\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.cluster import KMeans\n",
    "from collections import Counter\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-2-865d40cfe764>, line 43)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-2-865d40cfe764>\"\u001b[0;36m, line \u001b[0;32m43\u001b[0m\n\u001b[0;31m    print \"Accuracy {:.3}\".format(accuracy_score(y_true, y_pred))\u001b[0m\n\u001b[0m                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# helper functions\n",
    "def get_sentiment_data(file_path):\n",
    "    X = []\n",
    "    y = []\n",
    "    data_ = load_files(file_path, random_state=41)\n",
    "    for label, data in zip(data_.target, data_.data):\n",
    "        # FILTER OUT EMTPY REVIEWS \n",
    "        if data:\n",
    "            X.append(data)\n",
    "            y.append(label)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "def tokenize_reviews(X):\n",
    "    reg_tokenizer = RegexpTokenizer(pattern=r'\\w+')\n",
    "    X_tokenized = []\n",
    "    for review in X:\n",
    "        x_sent = []\n",
    "        for sentence in review.split(\"\\n\"):\n",
    "            x_sent.extend(reg_tokenizer.tokenize(sentence))\n",
    "        X_tokenized.append(x_sent)\n",
    "    return X_tokenized\n",
    "\n",
    "def get_bag_of_words(X):\n",
    "    vectorizer = CountVectorizer(stop_words = \"english\")\n",
    "    vectorizer.fit_transform(X)\n",
    "    X_count = vectorizer.transform(X)\n",
    "    return X_count\n",
    "\n",
    "def buildSentanceVector(text, size):\n",
    "    vec = np.zeros(size).reshape((1, size))\n",
    "    count = 0.\n",
    "    for word in text:\n",
    "        try:\n",
    "            vec += imdb_w2v[word].reshape((1, size))\n",
    "            count += 1.\n",
    "        except KeyError:\n",
    "            continue\n",
    "    if count != 0:\n",
    "        vec /= count\n",
    "    return vec\n",
    "\n",
    "def print_metrics(y_true, y_pred):\n",
    "    print (\"Accuracy {:.3}\").format(accuracy_score(y_true, y_pred))\n",
    "    print (\"Precison {:.3}\").format(precision_score(y_true, y_pred))\n",
    "    print (\"Recall {:.3}\").format(recall_score(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Data Engineering functions used to greatly speed up model optimization \n",
    "def persist_cv_splits(X, y, n_cv_iter=5, name='data', suffix=\"_cv_%03d.pkl\", test_size=0.25, random_state=None):\n",
    "    \"\"\"Materialize randomized train test splits of a dataset.\"\"\"\n",
    "\n",
    "    cv = ShuffleSplit(X.shape[0], \n",
    "                      n_iter=n_cv_iter,\n",
    "                      test_size=test_size, \n",
    "                      random_state=random_state)\n",
    "    \n",
    "    cv_split_filenames = []\n",
    "\n",
    "    for i, (train, test) in enumerate(cv):\n",
    "        \n",
    "        cv_fold = (X[train], y[train], X[test], y[test])\n",
    "        cv_split_filename = name + suffix % i\n",
    "        cv_split_filename = os.path.abspath(cv_split_filename)\n",
    "        joblib.dump(cv_fold, cv_split_filename)\n",
    "        cv_split_filenames.append(cv_split_filename)\n",
    "    \n",
    "    return cv_split_filenames\n",
    "\n",
    "def compute_evaluation(cv_split_filename, model, params):\n",
    "    \"\"\"Function executed by a worker to evaluate a model on a CV split\"\"\"\n",
    "    # All module imports should be executed in the worker namespace\n",
    "    from sklearn.externals import joblib\n",
    "\n",
    "    X_train, y_train, X_validation, y_validation = joblib.load(\n",
    "        cv_split_filename, mmap_mode='c')\n",
    "    \n",
    "    model.set_params(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    validation_score = model.score(X_validation, y_validation)\n",
    "    return validation_score\n",
    "\n",
    "def grid_search(lb_view, model, cv_split_filenames, param_grid):\n",
    "    \"\"\"Launch all grid search evaluation tasks.\"\"\"\n",
    "    all_tasks = []\n",
    "    all_parameters = list(ParameterGrid(param_grid))\n",
    "    \n",
    "    for i, params in enumerate(all_parameters):\n",
    "        task_for_params = []\n",
    "        \n",
    "        for j, cv_split_filename in enumerate(cv_split_filenames):    \n",
    "            t = lb_view.apply(\n",
    "                compute_evaluation, cv_split_filename, model, params)\n",
    "            task_for_params.append(t) \n",
    "        \n",
    "        all_tasks.append(task_for_params)\n",
    "        \n",
    "    return all_parameters, all_tasks\n",
    "\n",
    "def progress(tasks):\n",
    "    return np.mean([task.ready() for task_group in tasks\n",
    "                                 for task in task_group])\n",
    "\n",
    "def find_bests(all_parameters, all_tasks, n_top=5):\n",
    "    \"\"\"Compute the mean score of the completed tasks\"\"\"\n",
    "    mean_scores = []\n",
    "    \n",
    "    for param, task_group in zip(all_parameters, all_tasks):\n",
    "        scores = [t.get() for t in task_group if t.ready()]\n",
    "        if len(scores) == 0:\n",
    "            continue\n",
    "        mean_scores.append((np.mean(scores), param))\n",
    "                   \n",
    "    return sorted(mean_scores, reverse=True)[:n_top]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In my [previous attempts](https://github.com/AlexanderPhysics/DataBeast/blob/master/NaturalLanguageProcessing_NLP/FINAL_PROJECT_PRESENTATION.ipynb) at building a sentiment analysis model, I wasn't able to improve accracy much beyond 83%. The optmized machine learning ensemble that I built (comprised of Logistic Regression, SVC, and Navie Bayes) was only able to increase accuracy by 1%! These results told me that my optmized models simply weren't able to identify any additional signal from my bag-of-words repesentation of the IMDB reivews. More ingenuity was needed for feature engineering!\n",
    "\n",
    "#### Word Context \n",
    "\n",
    "The problem with bag-of-words representation is that it doesn't capture any of the word's context. For instance, the word \"sick\" can have either positive or negative sentiment depending on the context in which it is used. \"That movie was sick!\" and \"That move made me sick!\" express polar opposite sentiment. Also, the bag-of-words representation only considers the frequency in which words appear in each review. In this example, the only differing words are \"was\" in the first sentance and \"made me\" in the second sentance. Not very informative. \n",
    "\n",
    "#### Curse of Dimensionality \n",
    "\n",
    "In the bag-of-words representation of text, every single unique word becomes a feature in the feature set. Typically resulting in a feature set whos size is in the thousands or even tens of thousands! Feature sets of this vastness create problems for machine learning classifers. Data sets this huge exist in a vector space where the majority of points are located far away from the origin, near the surface of the hyper-volume that they create. This results in sparsity in the data matrix,causing the performance of machine learning models to suffer. \n",
    "\n",
    "####  Feature Engineering with Word2Vec\n",
    "\n",
    "Word2Vec is an artifical neural network that address the issues of word context and high dimensions. Word2Vec takes a word with a text representation and transforms it into a vector representation in a real-value vector space. The user can choose the dimensionality of that vector space, thus choosing to reduce the feature space by an order of magnitude or more. Additionally, Word2Vec also learns the contextual relationshps of each word and their placement in the vector space corresponds to their semantic meaning. For instance, vec(\"King\") + vec(\"Queen\") - vec(\"Man\") = vec(\"Woman\"). \n",
    "\n",
    "More information about Word2Vec can be found here:\n",
    "\n",
    "[Original paper written by google](https://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf)\n",
    "\n",
    "[A more mathamatical exposure](http://www-personal.umich.edu/~ronxin/pdf/w2vexp.pdf)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment\n",
    "\n",
    "We will explore wheather or not Word2Vec can provide significant improvements on the bag-of-words represenation of imdb movie reviews. In the link to my previous attempts, you can see in cell 114 that the ensemble had the best performance with an $\\textbf{accuracy}$ of $\\textbf{0.8492}$ and a $\\textbf{f1 score}$ of $\\textbf{0.8270}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load and pre-process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "file_path = '/Users/Alexander/Documents/Data/sentiment/data/imdb1/'\n",
    "X, Y = get_sentiment_data(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, Y = shuffle(X, Y)\n",
    "X_token = tokenize_reviews(X)\n",
    "# shuffle and split data for machine learning\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_token, \n",
    "                                                    Y, \n",
    "                                                    test_size=0.20, \n",
    "                                                    random_state=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# explore other hyper-parameters\n",
    "n_dim = 300\n",
    "n_cores = 3\n",
    "floor_freq = 5\n",
    "n_iter = 30\n",
    "# 1 -> softmax (best so far), 0 -> negative sampling\n",
    "# which to use when\n",
    "classifier = 1\n",
    "# Initialize model and build vocab\n",
    "imdb_w2v = Word2Vec(size=n_dim, \n",
    "                    workers=n_cores,\n",
    "                    min_count=floor_freq,\n",
    "                    hs=classifier,\n",
    "                   iter=n_iter)\n",
    "imdb_w2v.build_vocab(X_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# train word2vec over train reviews \n",
    "imdb_w2v.train(X_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# now words have a vector representaion in a 300 dim vector space\n",
    "imdb_w2v['big'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build Review Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build a vector representation for each review\n",
    "# for each review, build sentance vectors and average them to get the review vector\n",
    "# then scale each review vector to have zero mean and unit standard deviation (Normal Distribution)\n",
    "X_train_vectors = np.concatenate([buildSentanceVector(review, n_dim) for review in X_train])\n",
    "X_train_vectors = scale(X_train_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# build test set vectors as well\n",
    "X_test_vectors = np.concatenate([buildSentanceVector(review, n_dim) for review in X_test])\n",
    "X_test_vectors = scale(X_test_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optmize Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "digits_split_filenames = persist_cv_splits(X_train_vectors, np.array(Y_train), name='imdb', random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!ipcluster stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!ipcluster start -n=3 --daemon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "client = Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lb_view = client.load_balanced_view()\n",
    "model = SVC()\n",
    "svc_params = {'C': np.logspace(-1, 2, 4),\n",
    "              'gamma': np.logspace(-4, 0, 5),\n",
    "             'kernel': [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]}\n",
    "\n",
    "all_parameters, all_tasks = grid_search(\n",
    "   lb_view, model, digits_split_filenames, svc_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Tasks completed: {0}%\".format(100 * progress(all_tasks)))\n",
    "pprint(find_bests(all_parameters, all_tasks))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The combination of hyperparameters that leads to the best performance, in terms of accuracy, for SVM is located in the top row. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!ipcluster stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training machine learning classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# consider optimzing gbc\n",
    "# model = GradientBoostingClassifier(n_estimators=1000)\n",
    "# optimized svm\n",
    "model = SVC(C=10.0, gamma=0.0001, kernel=\"rbf\", probability=True)\n",
    "#model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model.fit(X_train_vectors, Y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print_metrics(Y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred_prob = model.predict_proba(X_test_vectors)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fpr,tpr,_ = roc_curve(Y_test, y_pred_prob)\n",
    "roc_auc = auc(fpr,tpr)\n",
    "plt.plot(fpr,tpr,label='area = %.2f' %roc_auc)\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.title(\"ROC Curve: Sentiment\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.legend(loc='lower right')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "\n",
    "After having leveraged Word2Vec, we see that the resulting performance is actually slightly worse than my previous attempts. One possible reason for this is that the review vectors are averaged sentance vectors. This means that even a positive review can be labeld negative because more sentances expressed negative sentiment than positive sentiment. \n",
    "    \n",
    "A possible solution to this problem is to leverage doc2vec which can better capture the sentiment of an entire reivew, without having to take averages of sentance vectors.\n",
    "\n",
    "The most convincing reason for this performance is the small sample size used to train Word2Vec. It is well known that neural networks need significantly large training sets to achieve great performance. Here, I insisted on using the same 2000 review IMBD data set so that I can compare the work in this notebook with my previous attempts. We can see from this preliminary experiment that Word2Vec shows promise but more data is needed! "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
