{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reset -f -s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/Users/Alexander'"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import WordNet_Lexicon as wn\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet as wnet\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "from nltk import RegexpTokenizer\n",
    "from sklearn import cross_validation\n",
    "from time import time\n",
    "from sklearn.svm import SVC\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#FLOW\n",
    "##Preprocessing Data\n",
    "    - Clearning data\n",
    "    - Spliting into Training and Test Sets\n",
    "    - KFold\n",
    "    \n",
    "##FIT MODELS\n",
    "    - Logistic Regression\n",
    "    - Multinomial Navie Bayes\n",
    "    - SVM\n",
    "    \n",
    "##Ensemble\n",
    "    - Tune Hyperparameters\n",
    "    - Resutls\n",
    "    \n",
    "##Word Sentiment\n",
    "    - WordNet\n",
    "    - Sentiment Lexicon\n",
    "    \n",
    "##Appendix\n",
    "    -Functions\n",
    "    -Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Preprocessing Data\n",
    "    - Clearning data\n",
    "    - Spliting into Training and Test Sets\n",
    "    - KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# LOAD MOVIE REVIEWS\n",
    "sentiment = load_files('/Users/Alexander/Downloads/review_polarity/txt_sentoken', random_state=41)\n",
    "\n",
    "# FILTER OUT EMTPY REVIEWS \n",
    "X = []\n",
    "y = []\n",
    "for label,data in zip(sentiment.target, sentiment.data):\n",
    "    if data:\n",
    "        X.append(data)\n",
    "        y.append(label)\n",
    "X  = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# COUNTVECTORIZER/ UNIGRAM DATA\n",
    "vectorizer = CountVectorizer(stop_words = \"english\")\n",
    "vectorizer.fit_transform(X)\n",
    "X_count = vectorizer.transform(X)\n",
    "\n",
    "## TF-IDF/ UNIGRAM DATA\n",
    "n_features = 5000\n",
    "tfidf = TfidfVectorizer(max_features=n_features,\n",
    "                        stop_words='english')\n",
    "X_tfidf = tfidf.fit_transform(X)\n",
    "\n",
    "## SPLIT CountVect DATA INTO TRAIN AND TEST SETS \n",
    "split = 1400\n",
    "## TRAIN DATA 0.80\n",
    "Xu1c = X_count[:split]\n",
    "Xu2c = X_count[split:]\n",
    "\n",
    "## SPLIT TFIDF DATA INTO TRAIN AND TEST SETS \n",
    "## TRAIN DATA 0.80\n",
    "Xu1tf = X_tfidf[:split]\n",
    "Xu2tf= X_tfidf[split:]\n",
    "\n",
    "## TEST DATA 0.20\n",
    "yu1 = y[:split]\n",
    "yu2 = y[split:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#FIT MODELS\n",
    "    - Logistic Regression\n",
    "    - Multinomial Navie Bayes\n",
    "    - SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#Hypothesis One:\n",
    "##TFIDF Vectorizer DOES NOT lead to good model perfomance\n",
    "\n",
    "$$ TFIDF = ( 1 + log(TF_{t,d}) )~~log( \\frac{N}{DF_{t}} )$$\n",
    "\n",
    "$TF_{t,d} = $ number of occurances of term (t) in a given doc (d)\n",
    "\n",
    "$DF_{t} =$  number of documents (d) that term (t) appears in \n",
    "\n",
    "$N = $ total number of documents in corpus \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# CREATE DATAFRAME TO DISPLAYS METRIC RESULTS\n",
    "# Acc_count, Acc_TF, F1_count,F1_TF\n",
    "lr_test  = [0.838, 0.811,0.815,0.759]\n",
    "mnb_test = [0.794, 0.760, 0.756,0.667]\n",
    "svc_test = [0.818,0.55205811138,0.787535410765,\"error\"]\n",
    "names = [\"Logistgic Regression\",\"Multinomial Navie Bayes\", \"SVC\"]\n",
    "data = [lr_test,mnb_test,svc_test]\n",
    "table = pd.DataFrame(data, index = names)\n",
    "table.columns = [\"Acc_count\",\"Acc_TF\",\"F1_count\",\"F1_TF\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'character' appears in 922 movie reviews\n",
      "'dialogue'  appears in 369 movie reviews\n",
      "'annoying'  appears in 148 movie reviews\n"
     ]
    }
   ],
   "source": [
    "#TOKENIZE REVIEWS WITH REGEX IN ORDER TO PRESERVE CONTRACTIONS\n",
    "tokenizer = RegexpTokenizer(r\"[\\w']+\")\n",
    "X_tokens = [tokenizer.tokenize(review) for review in X]\n",
    "\n",
    "count1 = 0\n",
    "for doc in X_tokens:\n",
    "    if 'character' in doc:\n",
    "        count1 +=1\n",
    "print \"'character' appears in {} movie reviews\".format(count1)\n",
    "count2 = 0\n",
    "for doc in X_tokens:\n",
    "    if 'dialogue' in doc:\n",
    "        count2 +=1\n",
    "print \"'dialogue'  appears in {} movie reviews\".format(count2)\n",
    "count3 = 0\n",
    "for doc in X_tokens:\n",
    "    if 'annoying' in doc:\n",
    "        count3 +=1\n",
    "print \"'annoying'  appears in {} movie reviews\".format(count3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Conclusion One:\n",
    "\n",
    "In movie reviews, the most common words tend to be the most descriptive of the reviewer's sentiment, words like 'character', 'annoying, 'dialogue'. Yet, TFIDF will down weight their importance because they tend to have a high document frequency. \n",
    "\n",
    "    1 CountVectorizers leads to better predictions than TF-IDF. \n",
    "    2 Use CountVectorizer for this data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Acc_count</th>\n",
       "      <th>Acc_TF</th>\n",
       "      <th>F1_count</th>\n",
       "      <th>F1_TF</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Logistgic Regression</th>\n",
       "      <td>0.838</td>\n",
       "      <td>0.811000</td>\n",
       "      <td>0.815000</td>\n",
       "      <td>0.759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Multinomial Navie Bayes</th>\n",
       "      <td>0.794</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.756000</td>\n",
       "      <td>0.667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SVC</th>\n",
       "      <td>0.818</td>\n",
       "      <td>0.552058</td>\n",
       "      <td>0.787535</td>\n",
       "      <td>error</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Acc_count    Acc_TF  F1_count  F1_TF\n",
       "Logistgic Regression         0.838  0.811000  0.815000  0.759\n",
       "Multinomial Navie Bayes      0.794  0.760000  0.756000  0.667\n",
       "SVC                          0.818  0.552058  0.787535  error"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# COMPARING RESULTS FROM COUNT VECTORIZATION AND TF-IDF VECTORIZATION\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Hypothesis Two:\n",
    "###Bigrams lead to better model performance because these common words need context to better signal sentiment (I.e. Great Dialouge, Flat Characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2), \n",
    "                                    token_pattern=r'\\b\\w+\\b', \n",
    "                                    min_df=1)\n",
    "X_2 = bigram_vectorizer.fit_transform(X).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression  Training Data\n",
      "Accuracy: 0.81 (+/- 0.01) \n",
      "Precison: 0.83 (+/- 0.03) \n",
      "Recall:   0.74 (+/- 0.03)\n",
      "LogisticRegression  Testing Data\n",
      "Accuracy:   0.794188861985\n",
      "F1-Score:   0.752186588921\n"
     ]
    }
   ],
   "source": [
    "## SPLIT CountVect DATA INTO TRAIN AND TEST SETS \n",
    "split = 1400\n",
    "## TRAIN DATA 0.80\n",
    "Xub1 = X_2[:split]\n",
    "Xub2= X_2[split:]\n",
    "## TEST DATA 0.20\n",
    "yub1 = y[:split]\n",
    "yub2 = y[split:]\n",
    "\n",
    "logit = LogisticRegression()\n",
    "acc_score, pre_score,rec_score = fit_predict_model2(logit,Xub1,yub1)\n",
    "print logit.__class__.__name__ + \"  Training Data\"\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f) \" % (acc_score[0], acc_score[1]))\n",
    "print(\"Precison: %0.2f (+/- %0.2f) \" % (pre_score[0], pre_score[1]))\n",
    "print(\"Recall:   %0.2f (+/- %0.2f)\" % (rec_score[0], rec_score[1]))\n",
    "\n",
    "yhat = logit.predict(Xub2)\n",
    "print logit.__class__.__name__ + \"  Testing Data\"\n",
    "print \"Accuracy:  \", accuracy_score(yub2,yhat)\n",
    "print \"F1-Score:  \", f1_score(yub2,yhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB  Training Data\n",
      "Accuracy: 0.80 (+/- 0.02) \n",
      "Precison: 0.85 (+/- 0.06) \n",
      "Recall:   0.69 (+/- 0.08)\n",
      "MultinomialNB  Testing Data\n",
      "Accuracy:   0.769975786925\n",
      "F1-Score:   0.677966101695\n"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "acc_score1, pre_score1,rec_score1 = fit_predict_model2(nb,Xub1,yub1)\n",
    "print nb.__class__.__name__ + \"  Training Data\"\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f) \" % (acc_score1[0], acc_score1[1]))\n",
    "print(\"Precison: %0.2f (+/- %0.2f) \" % (pre_score1[0], pre_score1[1]))\n",
    "print(\"Recall:   %0.2f (+/- %0.2f)\" % (rec_score1[0], rec_score1[1]))\n",
    "\n",
    "yhat1 = nb.predict(Xub2)\n",
    "print nb.__class__.__name__ + \"  Testing Data\"\n",
    "print \"Accuracy:  \", accuracy_score(yub2,yhat1)\n",
    "print \"F1-Score:  \", f1_score(yub2,yhat1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Conclusion Two:\n",
    "    1 Bigrams DO NOT lead to better predictions, in fact the predictions are slightly worse. \n",
    "    2 Word Sentiment may not be captured in Bigrams \n",
    "    3 N-grams greater than N = 1 may be overfitting the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Consider...\n",
    "\n",
    "##Models have instrinsic bias based on the underlying assumptions \n",
    "\n",
    "$$ Bias[\\hat{f}(x)] = E[\\hat{f}(x)]~-~f(x) $$\n",
    "\n",
    "###Logistic Regression\n",
    "    - Exponentiated beta's provide odds of a data point belongs to a certain class\n",
    "$$odds = \\frac{P(Y = 1)}{1 - P(Y = 1)} = e^{\\beta_0 + \\beta_1 X_1 + \\cdots + \\beta_p X_p}$$\n",
    "\n",
    "###SVM\n",
    "    - Data can be linearly seperated by a decision boundary\n",
    "$$ |\\beta_0 + \\textbf{$\\beta^{T}$}\\textbf{x}| = 1 $$\n",
    "\n",
    "\n",
    "###Multinomial Navie Bayes\n",
    "    - Assumes that word occurances are independent of each other\n",
    "$$P_{\\theta}(C=c_{i}|x_{1}, x_{2}, ... , x_{n}) \\propto P_{\\theta}(x_{1} | c) P_{\\theta}(c) \\ldots P_{\\theta}(x_{d} | c) P_{\\theta}(c)$$\n",
    "###Discriminative vs. Generative\n",
    "    - Discriminative assume that classification can best be done by conditioning predictions prob on data: P(Y|X)\n",
    "    - Generative assume that classification can best be done by joining predictions prob with data: P(Y,X)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Ensemble\n",
    "\n",
    "##Hypothesis Three:\n",
    "    The misclassifiying effects of model bias can be mitigated by ensembling. \n",
    "\n",
    "##Two Types of Ensembling\n",
    "###Type 1: Majority Vote (Hard Voting)\n",
    "    - The classification with the most votes will be used for prediction. \n",
    "    \n",
    "$$ \\text{Clf}1 \\rightarrow 1 \\\\\n",
    "   \\text{Clf}2 \\rightarrow 1 \\\\\n",
    "   \\text{Clf}3 \\rightarrow 2 \\\\\n",
    "   \\text{label 1 is choosen}$$    \n",
    "    \n",
    "    \n",
    "###Type 2: Weighted Average (Soft Voting)\n",
    "    - Each model has it's prediction multiplied by an optimized weight. \n",
    "    \n",
    "$$ \\text{P}_{i} =~ [w_1 w_2 \\dots w_n]  \\begin{bmatrix} p_{i, 1} \\\\ p_{i, 2}\\\\ \\vdots \\\\ p_{i, n} \\end{bmatrix}=~ w_1 \\cdot p_{i, 1} + w_2 \\cdot p_{i, 2} + \\dots + w_n \\cdot p_{i, n}~~\\forall_{i}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# MODELS USED IN ENSEMBLE\n",
    "cf1 = LogisticRegression()\n",
    "cf2 = MultinomialNB()\n",
    "cf3 = SVC(kernel = \"linear\", \n",
    "          C = 0.01, \n",
    "          gamma = 0.001, \n",
    "          probability=True, \n",
    "          cache_size=400)\n",
    "\n",
    "#OPTIMIZED HYPERPARAMETERS FOR ENSEMBLE\n",
    "w1,w2,w3 = (3,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Logistic Regression\n",
      "Accuracy: 0.838571428571 \n",
      "F1_score: 0.815678572086\n",
      "Time Elapsed: 0.181627333164 minutes\n",
      "\n",
      "Multinomial Naive Bayes\n",
      "Accuracy: 0.809285714286 \n",
      "F1_score: 0.778036877807\n",
      "Time Elapsed: 0.302635165056 minutes\n",
      "\n",
      "SVC\n",
      "Accuracy: 0.832142857143 \n",
      "F1_score: 0.807047994421\n",
      "Time Elapsed: 52.9176993132 minutes\n",
      "\n",
      "Ensemble\n",
      "Accuracy: 0.849285714286 \n",
      "F1_score: 0.827092021923\n",
      "Time Elapsed: 107.991673565 minutes\n"
     ]
    }
   ],
   "source": [
    "# WEIGHTED AVERAGE (soft voting)\n",
    "start = time()\n",
    "np.random.seed(123)\n",
    "eclf = EnsembleClassifier(clfs=[cf1, cf2, cf3], voting='soft', weights=[w1,w2,w3])\n",
    "\n",
    "for clf, label in zip([cf1, cf2, cf3, eclf], ['Logistic Regression',  'Multinomial Naive Bayes','SVC', 'Ensemble']):\n",
    "    acc_score, f1_ = fit_predict_model2(clf,Xu1.todense(), yu1)\n",
    "    print\"\\n{}\".format(label)\n",
    "    print\"Accuracy: {} \".format(acc_score)\n",
    "    print\"F1_score: {}\".format(f1_)\n",
    "    #     print(\"Precison: %0.2f (+/- %0.2f) \" % (pre_score[0], pre_score[1]))\n",
    "    #     print(\"Recall:   %0.2f (+/- %0.2f)\" % (rec_score[0], rec_score[1]))\n",
    "    end = time()\n",
    "    print \"Time Elapsed: {} minutes\".format((end - start)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Logistgic Regression</th>\n",
       "      <th>Multinomial Navie Bayes</th>\n",
       "      <th>SVC</th>\n",
       "      <th>Ensemble</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Accuracy</th>\n",
       "      <td>0.838</td>\n",
       "      <td>0.809</td>\n",
       "      <td>0.832</td>\n",
       "      <td>0.8492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f1_Score</th>\n",
       "      <td>0.815</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.807</td>\n",
       "      <td>0.8270</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          Logistgic Regression  Multinomial Navie Bayes    SVC  Ensemble\n",
       "Accuracy                 0.838                    0.809  0.832    0.8492\n",
       "f1_Score                 0.815                    0.778  0.807    0.8270"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# CREATE DATAFRAME TO DISPLAYS METRIC RESULTS\n",
    "\n",
    "ensemble_acc = np.array([0.838,0.809,0.832,0.8492]).reshape(1,4)\n",
    "ensemble_f1 = np.array([0.815, 0.778,0.807,0.827]).reshape(1,4)\n",
    "names = [\"Accuracy\",\"f1_Score\"]\n",
    "data2 = np.array([ensemble_acc, ensemble_f1]).reshape(2,4)\n",
    "table2 = pd.DataFrame(data2, index = names)\n",
    "table2.columns = [\"Logistgic Regression\",\"Multinomial Navie Bayes\", \"SVC\", \"Ensemble\"]\n",
    "table2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Conclusion Three:\n",
    "###1 There is not a meaningful increase in scoring metrics through ensembling.\n",
    "###2 It seems that no more signel can be extracted from the Unigram data set. \n",
    "###3 In order to extract more signel from the data, further feature engineering is necessary. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Word Sentiment\n",
    "    - Create Word Seeds, pass into Synset seed generator\n",
    "    - Pass seed Synsets into Synset propagator\n",
    "    - Generate Positive and Negative Lexicon of word sentiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Hypothesis FOUR:\n",
    "###The sentiment in individual words will provide a refined analysis of sentiment. Which will lead to better predictions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## CREATE SEED WORDS THAT WILL BE USED TO FIND SYNSETS AND BUILD A POSITIVE \n",
    "## AND NEGATIVE LEXICON\n",
    "seed_pos = [\"excellent\",\"intense\",\\\n",
    "            \"good\",\"outstanding\",\\\n",
    "            \"badass\",\"positive\",\\\n",
    "            \"strong\",\"reliable\",\\\n",
    "           \"happy\",\"thoughtfull\",\\\n",
    "           \"cool\",\"great\",\\\n",
    "            \"brilliant\",\"exciting\"\\\n",
    "           \"creative\",\"imaginative\"]\n",
    "\n",
    "seed_neg = [\"negative\",\"flat\",\\\n",
    "            \"boring\",\"annoying\",\\\n",
    "            \"poor\",\"weak\",\\\n",
    "            \"rehash\",\"lazy\",\\\n",
    "            \"unimaginative\",\"uncreative\",\\\n",
    "            \"shit\",\"stupid\"\\\n",
    "           \"wrong\",\"unimaginative\",\\\n",
    "           \"monotone\",\"dumb\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating lexicon...\n",
      "created synset lexicon in 8.14681601524 seconds\n"
     ]
    }
   ],
   "source": [
    "## DIFFERENT SENSES OF EACH WORD\n",
    "senses = 4\n",
    "pos_list, neg_list = create_lexicon(seed_pos,seed_neg, senses )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transforming synsets to tokens...\n",
      "transfored synsets into tokens in 32.479667902 seconds\n",
      "Positive words: 6110,  Negative words: 4557\n"
     ]
    }
   ],
   "source": [
    "posWords, negWords = synsets_to_tokens(pos_list, neg_list )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "s = set()\n",
    "s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "review:  100\n",
      "time: 2.70994708538 minutes\n",
      "\n",
      "review:  200\n",
      "time: 3.30587316751 minutes\n",
      "\n",
      "review:  300\n",
      "time: 3.9287823995 minutes\n",
      "\n",
      "review:  400\n",
      "time: 4.48367596865 minutes\n",
      "\n",
      "review:  500\n",
      "time: 5.02569961548 minutes\n",
      "\n",
      "review:  600\n",
      "time: 5.62838586569 minutes\n",
      "\n",
      "review:  700\n",
      "time: 6.22958499988 minutes\n",
      "\n",
      "review:  800\n",
      "time: 6.79790723324 minutes\n",
      "\n",
      "review:  900\n",
      "time: 7.40518773397 minutes\n",
      "\n",
      "review:  1000\n",
      "time: 7.98564985196 minutes\n",
      "\n",
      "review:  1100\n",
      "time: 8.55944881837 minutes\n",
      "\n",
      "review:  1200\n",
      "time: 9.22328660091 minutes\n",
      "\n",
      "review:  1300\n",
      "time: 9.77217181921 minutes\n",
      "\n",
      "review:  1400\n",
      "time: 10.3539442658 minutes\n",
      "\n",
      "review:  1500\n",
      "time: 10.962987868 minutes\n",
      "\n",
      "review:  1600\n",
      "time: 11.5504461527 minutes\n",
      "\n",
      "review:  1700\n",
      "time: 12.1481976191 minutes\n",
      "\n",
      "review:  1800\n",
      "time: 12.7073778351 minutes\n",
      "time: 12.760322086 minutes\n"
     ]
    }
   ],
   "source": [
    "start2 = time()\n",
    "review_sentiment = []\n",
    "review_index = 0\n",
    "stopWords = stopwords.words()\n",
    "## TOKENIZE REVIEW AND REMOVE STOPWORDS\n",
    "X_token = []\n",
    "for x in X:\n",
    "    review_temp = []\n",
    "    review_token = RegexpTokenizer(\"[\\w]+\").tokenize(x)\n",
    "    for token in review_token:\n",
    "        if token not in stopWords:\n",
    "            review_temp.append(token)\n",
    "    X_token.append(review_temp)\n",
    "    \n",
    "## CHECK EACH WORD IN EACH REVIEW FOR SENTIMENT \n",
    "for review in X_token:\n",
    "    word_sentiment = []\n",
    "    words_in_review = len(review)\n",
    "    for word in review:\n",
    "        ## WORD FREQUENCY IN REVIEW\n",
    "        word_i_count_in_review= Counter(review)[word]\n",
    "        ## IF WORD IN NEG LEXICON, THEN MULTIPLY BY -1\n",
    "        if word in negWords:\n",
    "            word_sentiment.append(word_i_count_in_review/words_in_review * -1)\n",
    "        elif word in posWords:\n",
    "            word_sentiment.append(word_i_count_in_review/words_in_review)\n",
    "    end2 = time()\n",
    "    if review_index >0:\n",
    "        if (review_index%100 == 0):\n",
    "            end3 = time()\n",
    "            print \"\\nreview: \",review_index\n",
    "            print \"time: {} minutes\".format((end2 - start2)/60)\n",
    "    review_sentiment.append(np.sum(word_sentiment))\n",
    "    review_index += 1\n",
    "    \n",
    "end3 = time()\n",
    "print \"time: {} minutes\".format((end3 - start2)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### RELATIVE FREQUENCIES OF UNIGRAMS\n",
    "split = 1400\n",
    "## TRAIN DATA 0.80\n",
    "X1 = review_sentiment[:split]\n",
    "X2= review_sentiment[split:]\n",
    "## TEST DATA 0.20\n",
    "y1 = y[:split]\n",
    "y2 = y[split:]\n",
    "\n",
    "# SPLIT TRAIN DATA INTO TRAIN AND DEVELOPMENT SETS\n",
    "Xtrain,Xtest,ytrain,ytest = train_test_split(X1,y1,test_size = 0.20)\n",
    "Xtrain = np.array(Xtrain).reshape(len(Xtrain),1)\n",
    "Xtest = np.array(Xtest).reshape(len(Xtest),1)\n",
    "# FIT AND PREDICT USING LOGISTIC REGRESSION \n",
    "lr = LogisticRegression()\n",
    "lr.fit(np.array(Xtrain).reshape(1120,1),ytrain)\n",
    "ypred = lr.predict(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:   0.617857142857\n",
      "Precision:  0.75\n",
      "Recall:     0.104347826087\n",
      "F1-Score:   0.18320610687\n"
     ]
    }
   ],
   "source": [
    "print \"Accuracy:  \", accuracy_score(ytest,ypred)\n",
    "print \"Precision: \", precision_score(ytest,ypred)\n",
    "print \"Recall:    \", recall_score(ytest,ypred)\n",
    "print \"F1-Score:  \", f1_score(ytest,ypred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Conclusion Four:\n",
    "###The results are inconclusive. The accuracy is very close to random. Next time, account for words with neutral sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Classifer Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_predict_model(model, X,y):\n",
    "    acc = []\n",
    "    pre = []\n",
    "    rec = []\n",
    "\n",
    "    kf = KFold(n = len(y), n_folds = 10, random_state = 41,shuffle = True)\n",
    "    \n",
    "    for train_index, test_index in kf:\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        model.fit(X_train,y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        acc.append(accuracy_score(y_test,y_pred))\n",
    "        pre.append(precision_score(y_test,y_pred))\n",
    "        rec.append(recall_score(y_test,y_pred))\n",
    "    return model, np.mean(acc),  np.mean(pre), np.mean(rec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_predict_model2(model, X,y):\n",
    "    #CREATE LIST TO STORE METRIC SCORES\n",
    "    acc = []\n",
    "    pre = []\n",
    "    rec = []\n",
    "    f1 = []\n",
    "    # INITIATE KFOLD TO RANDOMIZE THE DATA INTO 10 TRAIN AND 10 TEST FOLDS\n",
    "    # RANDOMIZING DATA HELPS PREVENT SCORES FROM BEING DEPENDENT ON A RANDOM SPLIT\n",
    "    # KFOLD VALIDATION IS CHOOSEN OVER CROSS_VAL_SCORE (CVS)BECAUSE CSV\n",
    "    #    ONLY ALLOWS FOR ONE SCORING METRIC TO BE USED PER INSTANCE\n",
    "    # WHERE AS WITH KFOLD, SEVERAL SCORING METRICS CAN BE CALCULATED \n",
    "    #    IN ONE INPLEMENTATION OF KFOLD\n",
    "    kf = KFold(n = len(y), n_folds = 10, random_state = 41,shuffle = True)\n",
    "    \n",
    "    for train_index, test_index in kf:\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "\n",
    "        model.fit(X_train,y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        acc.append(accuracy_score(y_test,y_pred))\n",
    "        pre.append(precision_score(y_test,y_pred))\n",
    "        rec.append(recall_score(y_test,y_pred))\n",
    "        \n",
    "    return np.mean(acc), 2 *(np.mean(pre)* np.mean(rec))/ (np.mean(pre)+ np.mean(rec))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Ensembe Classifer Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.base import ClassifierMixin\n",
    "from sklearn.base import TransformerMixin\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.externals import six\n",
    "from sklearn.base import clone\n",
    "from sklearn.pipeline import _name_estimators\n",
    "import numpy as np\n",
    "import operator\n",
    "\n",
    "\n",
    "class EnsembleClassifier(BaseEstimator, ClassifierMixin, TransformerMixin):\n",
    "    \"\"\" Soft Voting/Majority Rule classifier for unfitted clfs.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    clfs : array-like, shape = [n_classifiers]\n",
    "      A list of classifiers.\n",
    "      Invoking the `fit` method on the `VotingClassifier` will fit clones\n",
    "      of those original classifiers that will be stored in the class attribute\n",
    "      `self.clfs_`.\n",
    "\n",
    "    voting : str, {'hard', 'soft'} (default='hard')\n",
    "      If 'hard', uses predicted class labels for majority rule voting.\n",
    "      Else if 'soft', predicts the class label based on the argmax of\n",
    "      the sums of the predicted probalities, which is recommended for\n",
    "      an ensemble of well-calibrated classifiers.\n",
    "\n",
    "    weights : array-like, shape = [n_classifiers], optional (default=`None`)\n",
    "      Sequence of weights (`float` or `int`) to weight the occurances of\n",
    "      predicted class labels (`hard` voting) or class probabilities\n",
    "      before averaging (`soft` voting). Uses uniform weights if `None`.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    classes_ : array-like, shape = [n_predictions]\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    >>> import numpy as np\n",
    "    >>> from sklearn.linear_model import LogisticRegression\n",
    "    >>> from sklearn.naive_bayes import GaussianNB\n",
    "    >>> from sklearn.ensemble import RandomForestClassifier\n",
    "    >>> clf1 = LogisticRegression(random_state=1)\n",
    "    >>> clf2 = RandomForestClassifier(random_state=1)\n",
    "    >>> clf3 = GaussianNB()\n",
    "    >>> X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])\n",
    "    >>> y = np.array([1, 1, 1, 2, 2, 2])\n",
    "    >>> eclf1 = VotingClassifier(clfs=[clf1, clf2, clf3], voting='hard')\n",
    "    >>> eclf1 = eclf1.fit(X, y)\n",
    "    >>> print(eclf1.predict(X))\n",
    "    [1 1 1 2 2 2]\n",
    "    >>> eclf2 = VotingClassifier(clfs=[clf1, clf2, clf3], voting='soft')\n",
    "    >>> eclf2 = eclf2.fit(X, y)\n",
    "    >>> print(eclf2.predict(X))\n",
    "    [1 1 1 2 2 2]\n",
    "    >>> eclf3 = VotingClassifier(clfs=[clf1, clf2, clf3],\n",
    "    ...                          voting='soft', weights=[2,1,1])\n",
    "    >>> eclf3 = eclf3.fit(X, y)\n",
    "    >>> print(eclf3.predict(X))\n",
    "    [1 1 1 2 2 2]\n",
    "    >>>\n",
    "    \"\"\"\n",
    "    # INITIATES ENSEMBLE CLASSIFER CLASS\n",
    "    def __init__(self, clfs, voting='hard', weights=None):\n",
    "        # STORES LIST OF PASSED IN CLASSIFERS INTO CLASS ATTRIBUTE\n",
    "        self.clfs = clfs\n",
    "        self.named_clfs = {key:value for key,value in _name_estimators(clfs)}\n",
    "        # STORES TYPE OF VOTING INTO CLASS ATTRIBUTE\n",
    "        self.voting = voting\n",
    "        # STORES USER PASSED IN CLASSIFER WEIGHTS INTO CLASS ATTRIBUTE\n",
    "        self.weights = weights\n",
    "        \n",
    "    # FITS CLASSIFER\n",
    "    def fit(self, X, y):\n",
    "        \"\"\" Fit the clfs.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
    "            Training vectors, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "\n",
    "        y : array-like, shape = [n_samples]\n",
    "            Target values.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "        \"\"\"\n",
    "        # ENSURES THAT LABELS ARE BINARY\n",
    "        if isinstance(y, np.ndarray) and len(y.shape) > 1 and y.shape[1] > 1:\n",
    "            raise NotImplementedError('Multilabel and multi-output'\\\n",
    "                                      ' classification is not supported.')\n",
    "        # ENSURES THAT ENSEMBLE TYPE IS DEFINED\n",
    "        if self.voting not in ('soft', 'hard'):\n",
    "            raise ValueError(\"Voting must be 'soft' or 'hard'; got (voting=%r)\"\n",
    "                             % voting)\n",
    "        # ENSURES THAT THE NUMBER OF CLASSIFIERS AND WEIGHTS ARE THE SAME\n",
    "        if self.weights and len(self.weights) != len(self.clfs):\n",
    "            raise ValueError('Number of classifiers and weights must be equal'\n",
    "                             '; got %d weights, %d clfs'\n",
    "                             % (len(self.weights), len(self.clfs)))\n",
    "        # LabelEncoder IS USED TO TRANSFORM NON-NUMERICAL LABELS\n",
    "        # TO NUMERICAL LABELS\n",
    "        self.le_ = LabelEncoder()\n",
    "        self.le_.fit(y)\n",
    "        self.classes_ = self.le_.classes_\n",
    "        self.clfs_ = []\n",
    "        # CLASSIFIERS ARE CLONED SO THAT FITTING IS NOT DOEN INPLACE\n",
    "        for clf in self.clfs:\n",
    "            fitted_clf = clone(clf).fit(X, self.le_.transform(y))\n",
    "            self.clfs_.append(fitted_clf)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\" Predict class labels for X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
    "            Training vectors, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        maj : array-like, shape = [n_samples]\n",
    "            Predicted class labels.\n",
    "        \"\"\"\n",
    "        if self.voting == 'soft':\n",
    "\n",
    "            maj = np.argmax(self.predict_proba(X), axis=1)\n",
    "\n",
    "        else:  # 'hard' voting\n",
    "            predictions = self._predict(X)\n",
    "\n",
    "            maj = np.apply_along_axis(\n",
    "                                      lambda x:\n",
    "                                      np.argmax(np.bincount(x,\n",
    "                                                weights=self.weights)),\n",
    "                                      axis=1,\n",
    "                                      arr=predictions)\n",
    "\n",
    "        maj = self.le_.inverse_transform(maj)\n",
    "        return maj\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\" Predict class probabilities for X.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
    "            Training vectors, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "\n",
    "        Returns\n",
    "        ----------\n",
    "        avg : array-like, shape = [n_samples, n_classes]\n",
    "            Weighted average probability for each class per sample.\n",
    "        \"\"\"\n",
    "        # CALCULATES THE WEIGHTED AVERAGE FOR EACH CLASS\n",
    "        avg = np.average(self._predict_probas(X), axis=0, weights=self.weights)\n",
    "        return avg\n",
    "\n",
    "    def transform(self, X):\n",
    "        \"\"\" Return class labels or probabilities for X for each estimator.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : {array-like, sparse matrix}, shape = [n_samples, n_features]\n",
    "            Training vectors, where n_samples is the number of samples and\n",
    "            n_features is the number of features.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        If `voting='soft'`:\n",
    "          array-like = [n_classifiers, n_samples, n_classes]\n",
    "            Class probabilties calculated by each classifier.\n",
    "        If `voting='hard'`:\n",
    "          array-like = [n_classifiers, n_samples]\n",
    "            Class labels predicted by each classifier.\n",
    "        \"\"\"\n",
    "        if self.voting == 'soft':\n",
    "            return self._predict_probas(X)\n",
    "        else:\n",
    "            return self._predict(X)\n",
    "\n",
    "    def get_params(self, deep=True):\n",
    "        \"\"\" Return estimator parameter names for GridSearch support\"\"\"\n",
    "        if not deep:\n",
    "            return super(EnsembleClassifier, self).get_params(deep=False)\n",
    "        else:\n",
    "            out = self.named_clfs.copy()\n",
    "            for name, step in six.iteritems(self.named_clfs):\n",
    "                for key, value in six.iteritems(step.get_params(deep=True)):\n",
    "                    out['%s__%s' % (name, key)] = value\n",
    "            return out\n",
    "\n",
    "    def _predict(self, X):\n",
    "        \"\"\" Collect results from clf.predict calls. \"\"\"\n",
    "        return np.asarray([clf.predict(X) for clf in self.clfs_]).T\n",
    "\n",
    "    def _predict_probas(self, X):\n",
    "        \"\"\" Collect results from clf.predict calls. \"\"\"\n",
    "        return np.asarray([clf.predict_proba(X) for clf in self.clfs_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Weight Optimizer for the Ensemble Classifer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## BRUTE FORCE: COMPARE THE MEAN AVERAGE OF ALL POSSIBLE WEIGHTS (FROM 1 TO 4) FOR ALL 3 CLASSIFIERS \n",
    "np.random.seed(123)\n",
    "start5 = time()\n",
    "df = pd.DataFrame(columns=('w1', 'w2','mean', 'std'))\n",
    "\n",
    "i = 0\n",
    "for w1 in range(1,4):\n",
    "    for w2 in range(1,4):\n",
    "        for w3 in range(1,4):\n",
    "            if len(set((w1,w2))) == 1: # skip if all weights are equal\n",
    "                continue\n",
    "            \n",
    "            eclf = EnsembleClassifier(clfs=[cf1, cf2], weights=[w1,w2])\n",
    "            scores = cross_validation.cross_val_score(\n",
    "                                            estimator=eclf,\n",
    "                                            X=Xu1c.todense(), \n",
    "                                            y=yu1, \n",
    "                                            cv=3, \n",
    "                                            scoring='accuracy',\n",
    "                                            n_jobs=1)\n",
    "            \n",
    "            df.loc[i] = [w1, w2, scores.mean(), scores.std()]\n",
    "            i += 1\n",
    "end5 = time()\n",
    "print \"time elapsed: {}\".format(end5 - start5)\n",
    "df.sort(columns=['mean', 'std'], ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Create Positive and Negative Synset Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_lexicon(pos_seed,neg_seed, num_senses):\n",
    "    '''\n",
    "    Input:  positive word seeds (list)\n",
    "            negative word seeds (list)\n",
    "            num of word senses to capture in synsets (int)\n",
    "    Output: positive word list\n",
    "            negative word list\n",
    "    '''\n",
    "    print \"creating lexicon...\"\n",
    "    start = time()\n",
    "    # PASS IN SEED WORDS TO CREATE SEED SYNETS\n",
    "    pos_words = wn.create_seed_synsets(pos_seed)\n",
    "    neg_words = wn.create_seed_synsets(neg_seed)\n",
    "    # PASS IN SEED SYNSETS TO SENSE PROPAGATER\n",
    "    pos = wn.wordnet_sense_propagate(pos_words, num_senses)\n",
    "    neg = wn.wordnet_sense_propagate(neg_words, num_senses)\n",
    "    end = time()\n",
    "    print \"created synset lexicon in {} seconds\".format(end - start)\n",
    "    # RETURN POSITIVE AND NEGATIVE SYNSETS LEXICON \n",
    "    return pos,neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Create Positive and Negative Word Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def synsets_to_tokens(pos,neg):\n",
    "    '''\n",
    "    Input:  positive synsets (list)\n",
    "            negative synsets (list)\n",
    "    Ouput:  positive lexicon (list)\n",
    "            negative lexicon (list)\n",
    "    '''\n",
    "    print \"transforming synsets to tokens...\"\n",
    "    start1 = time()\n",
    "    build_pos = []\n",
    "    lex_pos  = []\n",
    "    for synset_list in pos:\n",
    "        for synsets in synset_list:\n",
    "            for synset in synsets:\n",
    "                if synset.name not in build_pos:\n",
    "                    build_pos.append(synset.name)\n",
    "                    lex_pos.append(synset.lemma_names())\n",
    "                    \n",
    "    build_neg = []\n",
    "    lex_neg  = []\n",
    "    for synset_list in neg:\n",
    "        for synsets in synset_list:\n",
    "            for synset in synsets:\n",
    "                if synset.name not in build_neg:\n",
    "                    build_neg.append(synset.name)\n",
    "                    lex_neg.append(synset.lemma_names())\n",
    "                    \n",
    "    neg_words = []\n",
    "    for synsets in lex_neg:\n",
    "        for word in synsets:\n",
    "            if word not in neg_words:\n",
    "                neg_words.append(word)\n",
    "                \n",
    "    pos_words = []\n",
    "    for synsets in lex_pos:\n",
    "        for word in synsets:\n",
    "            if word not in pos_words:\n",
    "                pos_words.append(word)\n",
    "                \n",
    "    end1 = time()\n",
    "    print \"transfored synsets into tokens in {} seconds\"\\\n",
    "    .format(end1 - start1)\n",
    "    print \"Positive words: {0},  Negative words: {1}\"\\\n",
    "    .format(len(pos_words),len(neg_words))\n",
    "    \n",
    "    return pos_words, neg_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Create Lexicon of Positive and Negative Synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_lexicon(pos_seed,neg_seed, num_senses):\n",
    "    '''\n",
    "    Input:  positive word seeds (list)\n",
    "            negative word seeds (list)\n",
    "            num of word senses to capture in synsets (int)\n",
    "    Output: positive word list\n",
    "            negative word list\n",
    "    '''\n",
    "    print \"creating lexicon...\"\n",
    "    start = time()\n",
    "    # PASS IN SEED WORDS TO CREATE SEED SYNETS\n",
    "    pos_words = wn.create_seed_synsets(pos_seed)\n",
    "    neg_words = wn.create_seed_synsets(neg_seed)\n",
    "    # PASS IN SEED SYNSETS TO SENSE PROPAGATER\n",
    "    pos = wn.wordnet_sense_propagate(pos_words, num_senses)\n",
    "    neg = wn.wordnet_sense_propagate(neg_words, num_senses)\n",
    "    end = time()\n",
    "    print \"created synset lexicon in {} seconds\".format(end - start)\n",
    "    # RETURN POSITIVE AND NEGATIVE LEXICON \n",
    "    return pos,neg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Calculations\n",
    "###Applying CountVectorizer and TFIDF vectorizer to data and modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression  TRAINING SET\n",
      "Ave Accuracy:   0.838571428571\n",
      "Ave Precision:  0.833515383979\n",
      "Ave Recall:     0.798589163817\n"
     ]
    }
   ],
   "source": [
    "## LOGISTIC REGRESSION TRAINING ON COUNTVECTORIZER\n",
    "clf1 = LogisticRegression()\n",
    "clf1, ave_acc , ave_pre, ave_rec = fit_predict_model(clf1,Xu1c.todense(),yu1)\n",
    "print clf1.__class__.__name__ + \"  TRAINING SET\"\n",
    "print \"Ave Accuracy:  \", ave_acc\n",
    "print \"Ave Precision: \", ave_pre\n",
    "print \"Ave Recall:    \", ave_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression  TESTING SET\n",
      "Accuracy:   0.837772397094\n",
      "Precision:  0.831460674157\n",
      "Recall:     0.8\n",
      "F1-Score:   0.815426997245\n"
     ]
    }
   ],
   "source": [
    "## LOGISTIC REGRESSION TESTING ON COUNTVECTORIZER\n",
    "ypred1 = clf1.predict(Xu2c.todense())\n",
    "print clf1.__class__.__name__ + \"  TESTING SET\"\n",
    "print \"Accuracy:  \", accuracy_score(yu2,ypred1)\n",
    "print \"Precision: \", precision_score(yu2,ypred1)\n",
    "print \"Recall:    \", recall_score(yu2,ypred1)\n",
    "print \"F1-Score:  \", f1_score(yu2,ypred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression  TRAINING SET\n",
      "Ave Accuracy:   0.818571428571\n",
      "Ave Precision:  0.859259890135\n",
      "Ave Recall:     0.708849388884\n"
     ]
    }
   ],
   "source": [
    "## LOGISTIC REGRESSION TRAINING ON TF-IDF\n",
    "clf2 = LogisticRegression()\n",
    "clf2, ave_acc , ave_pre, ave_rec = fit_predict_model(clf2,Xu1tf.todense(),yu1)\n",
    "print clf2.__class__.__name__ + \"  TRAINING SET\"\n",
    "print \"Ave Accuracy:  \", ave_acc\n",
    "print \"Ave Precision: \", ave_pre\n",
    "print \"Ave Recall:    \", ave_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression  TESTING SET\n",
      "Accuracy:   0.811138014528\n",
      "Precision:  0.884892086331\n",
      "Recall:     0.664864864865\n",
      "F1-Score:   0.759259259259\n"
     ]
    }
   ],
   "source": [
    "## LOGISTIC REGRESSION TESTING ON TF-IDF\n",
    "ypred2 = clf2.predict(Xu2tf.todense())\n",
    "print clf2.__class__.__name__ + \"  TESTING SET\"\n",
    "print \"Accuracy:  \", accuracy_score(yu2,ypred2)\n",
    "print \"Precision: \", precision_score(yu2,ypred2)\n",
    "print \"Recall:    \", recall_score(yu2,ypred2)\n",
    "print \"F1-Score:  \", f1_score(yu2,ypred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB  TRAINING SET\n",
      "Ave Accuracy:   0.809285714286\n",
      "Ave Precision:  0.815175460168\n",
      "Ave Recall:     0.744134835573\n"
     ]
    }
   ],
   "source": [
    "## MULTINOMIAL NAVIE BAYES TRAINING ON COUNTVECTORIZER\n",
    "clf3 = MultinomialNB()\n",
    "clf3, ave_acc , ave_pre, ave_rec = fit_predict_model(clf3,Xu1c.todense(),yu1)\n",
    "print clf3.__class__.__name__ + \"  TRAINING SET\"\n",
    "print \"Ave Accuracy:  \", ave_acc\n",
    "print \"Ave Precision: \", ave_pre\n",
    "print \"Ave Recall:    \", ave_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB  TESTING SET\n",
      "Accuracy:   0.794188861985\n",
      "Precision:  0.80487804878\n",
      "Recall:     0.713513513514\n",
      "F1-Score:   0.756446991404\n"
     ]
    }
   ],
   "source": [
    "## MULTINOMIAL NAVIE BAYES TESTING ON COUNTVECTORIZER\n",
    "ypred3 = clf3.predict(Xu2c.todense())\n",
    "print clf3.__class__.__name__ + \"  TESTING SET\"\n",
    "print \"Accuracy:  \", accuracy_score(yu2,ypred3)\n",
    "print \"Precision: \", precision_score(yu2,ypred3)\n",
    "print \"Recall:    \", recall_score(yu2,ypred3)\n",
    "print \"F1-Score:  \", f1_score(yu2,ypred3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB  TRAINING SET\n",
      "Ave Accuracy:   0.781428571429\n",
      "Ave Precision:  0.87728525014\n",
      "Ave Recall:     0.59730114787\n"
     ]
    }
   ],
   "source": [
    "## MULTINOMIAL NAVIE BAYES TRAINING ON TF-IDF\n",
    "clf4 = MultinomialNB()\n",
    "clf4, ave_acc , ave_pre, ave_rec = fit_predict_model(clf4,Xu1tf.todense(),yu1)\n",
    "print clf4.__class__.__name__ + \"  TRAINING SET\"\n",
    "print \"Ave Accuracy:  \", ave_acc\n",
    "print \"Ave Precision: \", ave_pre\n",
    "print \"Ave Recall:    \", ave_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultinomialNB  TESTING SET\n",
      "Accuracy:   0.760290556901\n",
      "Precision:  0.883928571429\n",
      "Recall:     0.535135135135\n",
      "F1-Score:   0.666666666667\n"
     ]
    }
   ],
   "source": [
    "## MULTINOMIAL NAVIE BAYES TESTING ON TF-IDF\n",
    "ypred4 = clf4.predict(Xu2tf.todense())\n",
    "print clf4.__class__.__name__ + \"  TESTING SET\"\n",
    "print \"Accuracy:  \", accuracy_score(yu2,ypred4)\n",
    "print \"Precision: \", precision_score(yu2,ypred4)\n",
    "print \"Recall:    \", recall_score(yu2,ypred4)\n",
    "print \"F1-Score:  \", f1_score(yu2,ypred4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
