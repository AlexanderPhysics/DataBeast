{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# connect to cluster\n",
    "from os.path import expanduser\n",
    "from IPython.parallel import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "url_file = expanduser('~/.starcluster/ipcluster/SecurityGroup:@sc-my_cluster-us-east-1.json')\n",
    "sshkey = expanduser('~/.ssh/Amazon_AWS_DataGuy.pem') \n",
    "client = Client(url_file, \n",
    "                sshkey = sshkey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2015-10-18 13:19:13.256 [IPClusterStop] Stopping cluster [pid=745] with [signal=2]\r\n"
     ]
    }
   ],
   "source": [
    "!ipcluster stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!ipcluster start -n=3 --daemon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.parallel import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "len(client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dview = client.direct_view()\n",
    "len(client.ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "from sklearn.cross_validation import ShuffleSplit\n",
    "import os\n",
    "\n",
    "def persist_cv_splits(X, y, n_cv_iter=5, name='data',\n",
    "    suffix=\"_cv_%03d.pkl\", test_size=0.25, random_state=None):\n",
    "    \"\"\"Materialize randomized train test splits of a dataset.\"\"\"\n",
    "\n",
    "    cv = ShuffleSplit(X.shape[0], n_iter=n_cv_iter,\n",
    "        test_size=test_size, random_state=random_state)\n",
    "    cv_split_filenames = []\n",
    "    \n",
    "    for i, (train, test) in enumerate(cv):\n",
    "        cv_fold = (X[train], y[train], X[test], y[test])\n",
    "        cv_split_filename = name + suffix % i\n",
    "        cv_split_filename = os.path.abspath(cv_split_filename)\n",
    "        joblib.dump(cv_fold, cv_split_filename)\n",
    "        cv_split_filenames.append(cv_split_filename)\n",
    "    \n",
    "    return cv_split_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "true = [\"yes\",\n",
    "        \"True\",\n",
    "        \"True.\"]\n",
    "\n",
    "false = [\"no\",\n",
    "         \"False\",\n",
    "         \"False.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"~/DSCI6003-student/week4/exercise/data/churn.csv\",\n",
    "                 true_values = true, \n",
    "                 false_values = false)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "drop_col = [\"State\",\n",
    "            \"Account Length\",\n",
    "            \"Area Code\",\n",
    "            \"Phone\"]\n",
    "\n",
    "df.drop(drop_col, axis = 1, \n",
    "        inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.rename(columns={'Churn?':'Churn'}, \n",
    "          inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import numpy as np\n",
    "def smote(X, y, minority_weight=.5):\n",
    "    '''\n",
    "    generates new observations in minority class\n",
    "    so that output X, y have specified percentage of majority observations\n",
    "    '''\n",
    "    # compute number of new examples required\n",
    "    # True = 1, False = 0 \n",
    "    # If True is majority (>=.50) Then round(class_ratio) = 1.0 \n",
    "    # Else if False is majoirty (< .50) Then round(class_ratio) = 0.0\n",
    "    # For walk through, let majority class be False = 0\n",
    "    class_ratio = y.sum()/float(len(y))\n",
    "    # majority_class_label = 0.0\n",
    "    majority_class_label = round(class_ratio)\n",
    "    \n",
    "    # X_minority holds all obs that correspond to the True label (minority class)\n",
    "    X_minority = X[y!=majority_class_label]\n",
    "    # y_minority holds all True values (minority class)\n",
    "    y_minority = y[y!=majority_class_label]\n",
    "    min_count = len(X_minority)\n",
    "    maj_count = len(X) - min_count\n",
    "    # scaling factor, for minority_weight = 0.5, reduces to Maj_count = SF * Min_count\n",
    "    # the SF is the ratio between theh classes, muliplying by this will balance them\n",
    "    scaling_factor = (maj_count/float(min_count))*(minority_weight/(1-minority_weight))\n",
    "    # generate new_obs_targets so that the minority class will be balance with the majority class\n",
    "    new_observations_target = round(scaling_factor*min_count) - min_count\n",
    "\n",
    "    # train KNN\n",
    "    knn_model = KNeighborsClassifier(n_neighbors=int(round(len(X_minority)**.5)))\n",
    "    knn_model.fit(X_minority, y_minority)\n",
    "    if new_observations_target < len(X_minority):\n",
    "        sample_indices = np.random.choice(xrange(X_minority), \n",
    "                                          size=new_observations_target,\n",
    "                                          replace=False)\n",
    "        smote_samples = X_minority[sample_indices]\n",
    "    else:\n",
    "        smote_samples = X_minority\n",
    "    neighbors = knn_model.kneighbors(smote_samples)[1]\n",
    "    \n",
    "    # generate new samples\n",
    "    new_observations = np.empty((0,X.shape[1]))\n",
    "    while len(new_observations) < new_observations_target:\n",
    "        index = len(new_observations) % len(smote_samples)\n",
    "        neighbor_index = np.random.choice(neighbors[index])\n",
    "        neighbor = smote_samples[neighbor_index]\n",
    "        x = X_minority[index]\n",
    "        new_x = x + (neighbor - x)*np.random.random(size=X_minority.shape[1])\n",
    "        new_observations = np.vstack((new_observations, new_x))\n",
    "    minority_class_label = (majority_class_label + 1) % 2\n",
    "    X = np.vstack((X, new_observations))\n",
    "    y = np.hstack((y, np.array([minority_class_label]*len(new_observations))))\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# columns won't rescale unless dtyp = \"float\"\n",
    "df2 = df[df.columns[2:-1]].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale, MinMaxScaler\n",
    "# rescale columns to range (-1,1)\n",
    "# Do not include columns with boolean values\n",
    "df2[df.columns[2:-1]] = df2[df.columns[2:-1]].apply(lambda x: MinMaxScaler(feature_range=(-1,1)).fit_transform(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Move columns with boolean values back into dataframe\n",
    "df2[df.columns[0]] = df[df.columns[0]].values\n",
    "df2[df.columns[1]] = df[df.columns[1]].values\n",
    "df2[df.columns[-1]] = df[df.columns[-1]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# target values: churn/not churn --> true/false\n",
    "y = df2.Churn.values\n",
    "# feature data\n",
    "X = df2[df2.columns[:-1]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rank_ind = np.array([13,  3,  1, 14, 11,  4,  6,  7,  9,  0, 10, 12,  2,  5,  8, 15])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_top_features = df2[rank_ind[0:12]].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_smote, y_smote = smote(X_top_features,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "churn_split_filenames = persist_cv_splits(X_smote, \n",
    "                                          y_smote,\n",
    "                                          name='churn', \n",
    "                                          random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Future Note\n",
    "    Train on subset of data set\n",
    "    This grid search should only be done on 80% of the data\n",
    "    I would have to find a way to import the test set (20%)\n",
    "    into another notebook - since I insist on performing grid \n",
    "    searches in isolation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Users/Alexander/DSCI6006_Business/churn_cv_000.pkl',\n",
       " '/Users/Alexander/DSCI6006_Business/churn_cv_001.pkl',\n",
       " '/Users/Alexander/DSCI6006_Business/churn_cv_002.pkl',\n",
       " '/Users/Alexander/DSCI6006_Business/churn_cv_003.pkl',\n",
       " '/Users/Alexander/DSCI6006_Business/churn_cv_004.pkl']"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "churn_split_filenames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "\n",
    "    Each churn_cv_00*.pkl file has 4 churn_cv_000.pkl_0*.npy accompanied files save.\n",
    "    Each one is an array: Xtrain, Xtest, ytrain, ytest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--  1 Alexander  staff   276B Oct 18 13:19 churn_cv_000.pkl\r\n",
      "-rw-r--r--  1 Alexander  staff   432K Oct 18 13:19 churn_cv_000.pkl_01.npy\r\n",
      "-rw-r--r--  1 Alexander  staff    33K Oct 18 13:19 churn_cv_000.pkl_02.npy\r\n",
      "-rw-r--r--  1 Alexander  staff   144K Oct 18 13:19 churn_cv_000.pkl_03.npy\r\n",
      "-rw-r--r--  1 Alexander  staff    11K Oct 18 13:19 churn_cv_000.pkl_04.npy\r\n",
      "-rw-r--r--  1 Alexander  staff   276B Oct 18 13:19 churn_cv_001.pkl\r\n",
      "-rw-r--r--  1 Alexander  staff   432K Oct 18 13:19 churn_cv_001.pkl_01.npy\r\n",
      "-rw-r--r--  1 Alexander  staff    33K Oct 18 13:19 churn_cv_001.pkl_02.npy\r\n",
      "-rw-r--r--  1 Alexander  staff   144K Oct 18 13:19 churn_cv_001.pkl_03.npy\r\n",
      "-rw-r--r--  1 Alexander  staff    11K Oct 18 13:19 churn_cv_001.pkl_04.npy\r\n",
      "-rw-r--r--  1 Alexander  staff   276B Oct 18 13:19 churn_cv_002.pkl\r\n",
      "-rw-r--r--  1 Alexander  staff   432K Oct 18 13:19 churn_cv_002.pkl_01.npy\r\n",
      "-rw-r--r--  1 Alexander  staff    33K Oct 18 13:19 churn_cv_002.pkl_02.npy\r\n",
      "-rw-r--r--  1 Alexander  staff   144K Oct 18 13:19 churn_cv_002.pkl_03.npy\r\n",
      "-rw-r--r--  1 Alexander  staff    11K Oct 18 13:19 churn_cv_002.pkl_04.npy\r\n",
      "-rw-r--r--  1 Alexander  staff   276B Oct 18 13:19 churn_cv_003.pkl\r\n",
      "-rw-r--r--  1 Alexander  staff   432K Oct 18 13:19 churn_cv_003.pkl_01.npy\r\n",
      "-rw-r--r--  1 Alexander  staff    33K Oct 18 13:19 churn_cv_003.pkl_02.npy\r\n",
      "-rw-r--r--  1 Alexander  staff   144K Oct 18 13:19 churn_cv_003.pkl_03.npy\r\n",
      "-rw-r--r--  1 Alexander  staff    11K Oct 18 13:19 churn_cv_003.pkl_04.npy\r\n",
      "-rw-r--r--  1 Alexander  staff   276B Oct 18 13:19 churn_cv_004.pkl\r\n",
      "-rw-r--r--  1 Alexander  staff   432K Oct 18 13:19 churn_cv_004.pkl_01.npy\r\n",
      "-rw-r--r--  1 Alexander  staff    33K Oct 18 13:19 churn_cv_004.pkl_02.npy\r\n",
      "-rw-r--r--  1 Alexander  staff   144K Oct 18 13:19 churn_cv_004.pkl_03.npy\r\n",
      "-rw-r--r--  1 Alexander  staff    11K Oct 18 13:19 churn_cv_004.pkl_04.npy\r\n"
     ]
    }
   ],
   "source": [
    "ls -lh churn*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # insert cross validation data files into starcluster instance \n",
    "\n",
    "# ! starcluster put my_cluster --user sgeadmin churn_cv_00* /mnt/sgeadmin/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# %%px -t0\n",
    "# %%bash\n",
    "# scp /mnt/sgeadmin/churn_cv_00* node001:/mnt/sgeadmin/\n",
    "# scp /mnt/sgeadmin/churn_cv_00* node002:/mnt/sgeadmin/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compute_evaluation(cv_split_filename, model, params):\n",
    "    from sklearn.metrics import accuracy_score,f1_score\n",
    "    \"\"\"Function executed by a worker to evaluate a model on a CV split\"\"\"\n",
    "    # All module imports should be executed in the worker namespace\n",
    "    from sklearn.externals import joblib\n",
    "\n",
    "    X_train, y_train, X_validation, y_validation = joblib.load(\n",
    "        cv_split_filename)\n",
    "    \n",
    "    #, mmap_mode='c') 'c' = compressed file, \n",
    "    # this is the reason for \"memory-mapped' error\n",
    "    # LOOK INTO THIS!!!!\n",
    "    \n",
    "    model.set_params(**params)\n",
    "    model.fit(X_train, y_train)\n",
    "    #validation_score = model.score(X_validation, y_validation)\n",
    "    return f1_score(model.predict(X_validation),y_validation, average = \"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def grid_search(lb_view, model, cv_split_filenames, param_grid):\n",
    "    \"\"\"Launch all grid search evaluation tasks.\"\"\"\n",
    "    all_tasks = []\n",
    "    all_parameters = list(ParameterGrid(param_grid))\n",
    "    \n",
    "    for i, params in enumerate(all_parameters):\n",
    "        task_for_params = []\n",
    "        \n",
    "        for j, cv_split_filename in enumerate(cv_split_filenames):    \n",
    "            t = lb_view.apply(\n",
    "                compute_evaluation, cv_split_filename, model, params)\n",
    "            task_for_params.append(t) \n",
    "        \n",
    "        all_tasks.append(task_for_params)\n",
    "        \n",
    "    return all_parameters, all_tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# remote_filenames = ['/mnt/sgeadmin/' + filename.split('/')[-1] for filename in churn_split_filenames]\n",
    "# remote_filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "from sklearn.grid_search import ParameterGrid\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lb_view = client.load_balanced_view()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators = 1000,\n",
    "                                random_state = 41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "'max_leaf_nodes': [2,4,None],\n",
    "'min_samples_leaf': [2,4],\n",
    "\"max_features\": [6,10],\n",
    "\"max_depth\": [3,5,7],\n",
    "\"min_samples_split\": [2,4,8]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_parameters, all_tasks = grid_search(lb_view, \n",
    "                                        model, \n",
    "                                        churn_split_filenames, \n",
    "                                        params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def progress(tasks):\n",
    "    return np.mean([task.ready() for task_group in tasks\n",
    "                                 for task in task_group])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_bests(all_parameters, all_tasks, n_top=5):\n",
    "    \"\"\"Compute the mean score of the completed tasks\"\"\"\n",
    "    mean_scores = []\n",
    "    \n",
    "    for param, task_group in zip(all_parameters, all_tasks):\n",
    "        scores = [t.get() for t in task_group if t.ready()]\n",
    "        if len(scores) == 0:\n",
    "            continue\n",
    "        mean_scores.append((np.mean(scores), param))\n",
    "                   \n",
    "    return sorted(mean_scores, reverse=True)[:n_top]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasks completed: 62.2222222222%\n"
     ]
    }
   ],
   "source": [
    "print(\"Tasks completed: {0}%\".format(100 * progress(all_tasks)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tasks completed: 100.0%\n",
      "[(0.90377059998803499,\n",
      "  {'max_depth': 7,\n",
      "   'max_features': 10,\n",
      "   'max_leaf_nodes': None,\n",
      "   'min_samples_leaf': 2,\n",
      "   'min_samples_split': 4}),\n",
      " (0.90377059998803499,\n",
      "  {'max_depth': 7,\n",
      "   'max_features': 10,\n",
      "   'max_leaf_nodes': None,\n",
      "   'min_samples_leaf': 2,\n",
      "   'min_samples_split': 2}),\n",
      " (0.90359767009072844,\n",
      "  {'max_depth': 7,\n",
      "   'max_features': 6,\n",
      "   'max_leaf_nodes': None,\n",
      "   'min_samples_leaf': 2,\n",
      "   'min_samples_split': 8}),\n",
      " (0.90299533752766725,\n",
      "  {'max_depth': 7,\n",
      "   'max_features': 6,\n",
      "   'max_leaf_nodes': None,\n",
      "   'min_samples_leaf': 2,\n",
      "   'min_samples_split': 4}),\n",
      " (0.90299533752766725,\n",
      "  {'max_depth': 7,\n",
      "   'max_features': 6,\n",
      "   'max_leaf_nodes': None,\n",
      "   'min_samples_leaf': 2,\n",
      "   'min_samples_split': 2})]\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "print(\"Tasks completed: {0}%\".format(100 * progress(all_tasks)))\n",
    "pprint(find_bests(all_parameters, all_tasks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
