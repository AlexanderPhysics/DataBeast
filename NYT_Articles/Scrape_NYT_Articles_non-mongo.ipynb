{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests,json, time\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from bs4 import BeautifulSoup\n",
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://www.listchallenges.com/f/lists/d7aacdae-74bd-42ff-b397-b73905b5867b.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Our goal is to scape as many New York Times articles from their api as possible. The web scraper below is desgined to open a mongo client, where all the downloaded articles will be stored and make api calls. This web scraper will run indefinately until the user termintes the code's calls. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New York Times API Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def call_api(url, payload, p=0):\n",
    "    # Add the 'page' parameter to the payload.\n",
    "    payload['page'] = p\n",
    "\n",
    "    # Get the requested url. Error handling for bad requests should be done in\n",
    "    # the calling function.\n",
    "    return requests.get(url, params=payload)\n",
    "\n",
    "\n",
    "def get_response(r):\n",
    "    # Use json.loads to read the response text\n",
    "    raw = json.loads(r.text)\n",
    "\n",
    "    # Return the meta (hits, etc.) and docs (containing urls'n'stuff) back\n",
    "    return raw['response']['meta'], raw['response']['docs']\n",
    "\n",
    "\n",
    "def get_body_text(docs):\n",
    "\n",
    "    # Grab the url from each document, if it exists, then scrape each url for\n",
    "    # its body text. If we get any errors along the way, continue on to the\n",
    "    # next document / url to be scraped.\n",
    "    result = []\n",
    "    for d in docs:\n",
    "\n",
    "        # Make a copy of the doc's dictionary\n",
    "        doc = d.copy()\n",
    "\n",
    "        # If there's no url (not sure why this happens sometimes) then ditch it\n",
    "        if not doc['web_url']:\n",
    "            continue\n",
    "\n",
    "        # Scrape the doc's url, return a soup object with the url's text.\n",
    "        soup = get_soup(doc['web_url'])\n",
    "        if not soup:\n",
    "            continue\n",
    "\n",
    "        # Find all of the paragraphs with the correct class.\n",
    "        # This class tag is specific to NYT articles.\n",
    "        body = soup.find_all('p', class_= \"story-body-text story-content\")\n",
    "        if not body:\n",
    "            continue\n",
    "\n",
    "        # Join the resulting body paragraphs' text (returned in a list).\n",
    "        doc['body'] = '\\n'.join([x.get_text() for x in body])\n",
    "\n",
    "        print (doc['web_url'])\n",
    "        result.append(doc)\n",
    "\n",
    "    return result\n",
    "\n",
    "def get_soup(url):\n",
    "    # Header to be passed in to NYT when scraping article text.\n",
    "    agent  = 'DataWrangling/1.1 (http://zipfianacademy.com; '\n",
    "    agent += 'class@zipfianacademy.com)'\n",
    "    headers = {'user_agent': agent}\n",
    "\n",
    "    # Wrap in a try-except to prevent a maxTry connection error from erroring\n",
    "    # out the program. Return None if there are any issues.\n",
    "    try:\n",
    "        r = requests.get(url, headers=headers)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "    # Just in case there was a normal error returned. Pass back None.\n",
    "    if r.status_code != 200: return None\n",
    "\n",
    "    # Otherwise return a soupified object containing the url text encoded in\n",
    "    # utf-8. Will toss back errors on some pages without the encoding in place.\n",
    "    return BeautifulSoup(r.text.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Url for NYT dev api\n",
    "NYT_URL = 'http://api.nytimes.com/svc/search/v2/articlesearch.json'\n",
    "api_key_path = \"/Users/databeast03/.secret_keys/nyt_api_key.txt\"\n",
    "with open(api_key_path, 'r') as handle:\n",
    "    API_KEY = handle.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Request all of the newest articles matching the search term\n",
    "#             \"section_name\": ['U.S.', 'World', 'Business Day'],\n",
    "payload  = {'sort': 'newest',\n",
    "            'api-key': API_KEY}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "pages = [1,2]\n",
    "docs_list = []\n",
    "for page in pages:\n",
    "# Call the API with BaseURL + params and page\n",
    "    r = call_api(NYT_URL, payload, page)\n",
    "    meta, docs = get_response(r)\n",
    "    docs_body = get_body_text(docs)\n",
    "    docs_list.append(docs_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.nytimes.com/aponline/2018/09/25/us/ap-us-animal-shelter-pepper-spray.html'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[3][\"web_url\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(r.text.encode('utf-8'), 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scarp_articles_text(articles_df):\n",
    "    \n",
    "    # Unable false positive warning from Pandas dataframe manipulation\n",
    "    pd.options.mode.chained_assignment = None\n",
    "    \n",
    "    articles_df['article_text'] = 'NaN'\n",
    "    session = requests.Session()\n",
    "    \n",
    "    print('Scarping articles body text...'),\n",
    "    \n",
    "    for j in range(0, len(articles_df)):\n",
    "        \n",
    "        url = articles_df['url'][j]\n",
    "        req = session.get(url)\n",
    "        soup = BeautifulSoup(req.text, 'lxml')\n",
    "\n",
    "        # Get only HTLM tags with article content\n",
    "        # Articles through 1986 are found under different p tag \n",
    "        paragraph_tags = soup.find_all('p', class_= 'story-body-text story-content')\n",
    "        if paragraph_tags == []:\n",
    "            paragraph_tags = soup.find_all('p', itemprop = 'articleBody')\n",
    "\n",
    "        # Put together all text from HTML p tags\n",
    "        article = ''\n",
    "        for p in paragraph_tags:\n",
    "            article = article + ' ' + p.get_text()\n",
    "\n",
    "        # Clean article replacing unicode characters\n",
    "        article = article.replace(u'\\u2018', u\"'\").replace(u'\\u2019', u\"'\").replace(u'\\u201c', u'\"').replace(u'\\u201d', u'\"')\n",
    "\n",
    "        # Copy article's content to the dataframe\n",
    "        articles_df['article_text'][j] = article\n",
    "    \n",
    "    print('Done!')\n",
    "    \n",
    "    return articles_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "session = requests.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "req = session.get(docs[2][\"web_url\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(req.text, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_tags = soup.find_all('p', class_= 'story-body-text story-content')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph_tags = soup.find_all('p', itemprop = 'articleBody')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
