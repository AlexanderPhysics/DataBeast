{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, pymongo, json, time\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](http://www.listchallenges.com/f/lists/d7aacdae-74bd-42ff-b397-b73905b5867b.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Our goal is to scape as many New York Times articles from their api as possible. The web scraper below is desgined to open a mongo client, where all the downloaded articles will be stored and make api calls. This web scraper will run indefinately until the user termintes the code's calls. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New York Times Web Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def init_mongo_client():\n",
    "    # Initiate Mongo client\n",
    "    client = pymongo.MongoClient()\n",
    "\n",
    "    # Access database created for these articles\n",
    "    db = client.nyt2016\n",
    "    \n",
    "    # Access collection created for these articles\n",
    "    coll = db.articles\n",
    "\n",
    "    # Access articles collection in db and return collection pointer.\n",
    "    return db.articles\n",
    "\n",
    "\n",
    "def call_api(url, payload, p=0):\n",
    "    # Add the 'page' parameter to the payload.\n",
    "    payload['page'] = p\n",
    "\n",
    "    # Get the requested url. Error handling for bad requests should be done in\n",
    "    # the calling function.\n",
    "    return requests.get(url, params=payload)\n",
    "\n",
    "\n",
    "def get_response(r):\n",
    "    # Use json.loads to read the response text\n",
    "    raw = json.loads(r.text)\n",
    "\n",
    "    # Return the meta (hits, etc.) and docs (containing urls'n'stuff) back\n",
    "    return raw['response']['meta'], raw['response']['docs']\n",
    "\n",
    "\n",
    "def get_soup(url):\n",
    "    # Header to be passed in to NYT when scraping article text.\n",
    "    agent  = 'DataWrangling/1.1 (http://zipfianacademy.com; '\n",
    "    agent += 'class@zipfianacademy.com)'\n",
    "    headers = {'user_agent': agent}\n",
    "\n",
    "    # Wrap in a try-except to prevent a maxTry connection error from erroring\n",
    "    # out the program. Return None if there are any issues.\n",
    "    try:\n",
    "        r = requests.get(url, headers=headers)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "    # Just in case there was a normal error returned. Pass back None.\n",
    "    if r.status_code != 200: return None\n",
    "\n",
    "    # Otherwise return a soupified object containing the url text encoded in\n",
    "    # utf-8. Will toss back errors on some pages without the encoding in place.\n",
    "    return BeautifulSoup(r.text.encode('utf-8'))\n",
    "\n",
    "\n",
    "def get_body_text(docs):\n",
    "\n",
    "    # Grab the url from each document, if it exists, then scrape each url for\n",
    "    # its body text. If we get any errors along the way, continue on to the\n",
    "    # next document / url to be scraped.\n",
    "    result = []\n",
    "    for d in docs:\n",
    "\n",
    "        # Make a copy of the doc's dictionary\n",
    "        doc = d.copy()\n",
    "\n",
    "        # If there's no url (not sure why this happens sometimes) then ditch it\n",
    "        if not doc['web_url']:\n",
    "            continue\n",
    "\n",
    "        # Scrape the doc's url, return a soup object with the url's text.\n",
    "        soup = get_soup(doc['web_url'])\n",
    "        if not soup:\n",
    "            continue\n",
    "\n",
    "        # Find all of the paragraphs with the correct class.\n",
    "        # This class tag is specific to NYT articles.\n",
    "        body = soup.find_all('p', class_= \"story-body-text story-content\")\n",
    "        if not body:\n",
    "            continue\n",
    "\n",
    "        # Join the resulting body paragraphs' text (returned in a list).\n",
    "        doc['body'] = '\\n'.join([x.get_text() for x in body])\n",
    "\n",
    "        print (doc['web_url'])\n",
    "        result.append(doc)\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "def remove_previously_scraped(coll, docs):\n",
    "    # Check to see if the mongo collection already contains the docs returned\n",
    "    # from NYT. Return back a list of the ones that aren't in the collection to\n",
    "    # be scraped.\n",
    "    new_docs = []\n",
    "    for doc in docs:\n",
    "        # Check fo the document id in mongo. If it finds none, append to\n",
    "        # new_docs\n",
    "        cursor = articles.find({'_id': doc['_id']}).limit(1)\n",
    "        if not cursor.count() > 0:\n",
    "            new_docs.append(doc)\n",
    "\n",
    "    if new_docs == []:\n",
    "        return None\n",
    "\n",
    "    return new_docs\n",
    "\n",
    "\n",
    "def get_end_date(dt):\n",
    "    # String-ify the datetime object to YYYMMDD, which the NYT likes.\n",
    "    yr   = str(dt.year)\n",
    "    mon = '0' * (2 - len(str(dt.month))) + str(dt.month)\n",
    "    day = '0' * (2 - len(str(dt.day))) + str(dt.day)\n",
    "    return yr + mon + day\n",
    "\n",
    "\n",
    "def scrape_articles(coll, last_date):\n",
    "    page = 0\n",
    "    while page <= 199:\n",
    "        print ('Page:', page)\n",
    "\n",
    "        # Request all of the newest articles matching the search term\n",
    "        payload  = {'sort': 'newest',\n",
    "                    'end_date': get_end_date(last_date),\n",
    "                    'api-key': API_KEY}\n",
    "\n",
    "        # Call the API with BaseURL + params and page\n",
    "        r = call_api(NYT_URL, payload, page)\n",
    "\n",
    "        # Increment the page before we encounter any potential errors\n",
    "        page += 1\n",
    "\n",
    "        # Check to see if the metadata didn't come back. USUALLY happens if\n",
    "        # page > 100. When it does, reset the whole thing, roll the date back\n",
    "        # one day, sleep for a couple seconds, then keep going.\n",
    "        if r.status_code != 200:\n",
    "            page = 0\n",
    "            last_date += relativedelta(days=-1)\n",
    "            print ('End Date:', get_end_date(last_date))\n",
    "            print (r.status_code )\n",
    "            time.sleep(2)\n",
    "            continue\n",
    "            \n",
    "        # Get the meta data & documents from the request\n",
    "        meta, docs = get_response(r)\n",
    "\n",
    "        # Check if docs are already in the database\n",
    "        new_docs = remove_previously_scraped(coll, docs)\n",
    "        if not new_docs: continue\n",
    "\n",
    "        # Grab only the docs that have these tags\n",
    "        docs_with_body = get_body_text(new_docs)\n",
    "\n",
    "        for doc in docs_with_body:\n",
    "            try:\n",
    "                # Insert each doc into Mongo\n",
    "                coll.insert_one(doc)\n",
    "            except:\n",
    "                # If there's any error writing it in the db, just move along.\n",
    "                continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Url for NYT dev api\n",
    "NYT_URL = 'http://api.nytimes.com/svc/search/v2/articlesearch.json'\n",
    "api_key_path = \"/Users/databeast03/.secret_keys/nyt_api_key.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(api_key_path, 'r') as handle:\n",
    "    API_KEY = handle.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize db & collection\n",
    "articles = init_mongo_client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the initial end date (scraper starts at this date and moves back in\n",
    "# time sequentially)\n",
    "last_date = datetime.now() + relativedelta(days=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Page: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/databeast03/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:97: DeprecationWarning: count is deprecated. Use Collection.count_documents instead.\n"
     ]
    },
    {
     "ename": "ServerSelectionTimeoutError",
     "evalue": "localhost:27017: [Errno 61] Connection refused",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mServerSelectionTimeoutError\u001b[0m               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-238afb07e7cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Pass the database collection and initial end date into main function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mscrape_articles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marticles\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_date\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-24d17484ece8>\u001b[0m in \u001b[0;36mscrape_articles\u001b[0;34m(coll, last_date)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;31m# Check if docs are already in the database\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mnew_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mremove_previously_scraped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoll\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnew_docs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-24d17484ece8>\u001b[0m in \u001b[0;36mremove_previously_scraped\u001b[0;34m(coll, docs)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0;31m# new_docs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mcursor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marticles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'_id'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlimit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcursor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m             \u001b[0mnew_docs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pymongo/cursor.py\u001b[0m in \u001b[0;36mcount\u001b[0;34m(self, with_limit_and_skip)\u001b[0m\n\u001b[1;32m    767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    768\u001b[0m         return self.__collection._count(\n\u001b[0;32m--> 769\u001b[0;31m             cmd, self.__collation, session=self.__session)\n\u001b[0m\u001b[1;32m    770\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    771\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdistinct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pymongo/collection.py\u001b[0m in \u001b[0;36m_count\u001b[0;34m(self, cmd, collation, session)\u001b[0m\n\u001b[1;32m   1561\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1562\u001b[0m         \u001b[0;34m\"\"\"Internal count helper.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1563\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_socket_for_reads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mslave_ok\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1564\u001b[0m             res = self._command(\n\u001b[1;32m   1565\u001b[0m                 \u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pymongo/mongo_client.py\u001b[0m in \u001b[0;36m_socket_for_reads\u001b[0;34m(self, read_preference)\u001b[0m\n\u001b[1;32m   1097\u001b[0m         \u001b[0mtopology\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_topology\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1098\u001b[0m         \u001b[0msingle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopology\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescription\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtopology_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mTOPOLOGY_TYPE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSingle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1099\u001b[0;31m         \u001b[0mserver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtopology\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect_server\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mread_preference\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1101\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserver\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msock_info\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pymongo/topology.py\u001b[0m in \u001b[0;36mselect_server\u001b[0;34m(self, selector, server_selection_timeout, address)\u001b[0m\n\u001b[1;32m    222\u001b[0m         return random.choice(self.select_servers(selector,\n\u001b[1;32m    223\u001b[0m                                                  \u001b[0mserver_selection_timeout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m                                                  address))\n\u001b[0m\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m     def select_server_by_address(self, address,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pymongo/topology.py\u001b[0m in \u001b[0;36mselect_servers\u001b[0;34m(self, selector, server_selection_timeout, address)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m             server_descriptions = self._select_servers_loop(\n\u001b[0;32m--> 183\u001b[0;31m                 selector, server_timeout, address)\n\u001b[0m\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m             return [self.get_server_by_address(sd.address)\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pymongo/topology.py\u001b[0m in \u001b[0;36m_select_servers_loop\u001b[0;34m(self, selector, timeout, address)\u001b[0m\n\u001b[1;32m    197\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mnow\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mend_time\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 raise ServerSelectionTimeoutError(\n\u001b[0;32m--> 199\u001b[0;31m                     self._error_message(selector))\n\u001b[0m\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_opened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mServerSelectionTimeoutError\u001b[0m: localhost:27017: [Errno 61] Connection refused"
     ]
    }
   ],
   "source": [
    "# Pass the database collection and initial end date into main function\n",
    "scrape_articles(articles, last_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
